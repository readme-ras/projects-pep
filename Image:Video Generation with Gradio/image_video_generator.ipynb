{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¨ Image & Video Generation â€” Stable Diffusion + Gradio\n",
    "\n",
    "**Runtime: GPU (T4 recommended)**  \n",
    "Runtime â†’ Change runtime type â†’ T4 GPU\n",
    "\n",
    "### What this notebook does:\n",
    "- **Image generation** â€” Stable Diffusion XL (SDXL) via ğŸ¤— Diffusers\n",
    "- **Image-to-image** â€” transform existing images with a prompt\n",
    "- **Video generation** â€” AnimateDiff (text â†’ short MP4 clip)\n",
    "- **Gradio UI** â€” all features in one polished tabbed interface"
   ]
  },

  {
   "cell_type": "code",
   "metadata": {"id": "cell_install"},
   "source": [
    "# â”€â”€ Cell 1: Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q \\\n",
    "    diffusers>=0.27.0 \\\n",
    "    transformers>=4.40.0 \\\n",
    "    accelerate>=0.29.0 \\\n",
    "    safetensors>=0.4.0 \\\n",
    "    gradio>=4.26.0 \\\n",
    "    imageio[ffmpeg]>=2.34.0 \\\n",
    "    xformers \\\n",
    "    invisible-watermark \\\n",
    "    opencv-python-headless\n",
    "\n",
    "print('âœ… Dependencies installed')"
   ],
   "execution_count": null,
   "outputs": []
  },

  {
   "cell_type": "code",
   "metadata": {"id": "cell_imports"},
   "source": [
    "# â”€â”€ Cell 2: Imports & GPU check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import tempfile\n",
    "import os\n",
    "import gc\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    "    AnimateDiffPipeline,\n",
    "    MotionAdapter,\n",
    "    EulerDiscreteScheduler,\n",
    "    DDIMScheduler,\n",
    ")\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DTYPE  = torch.float16 if DEVICE == 'cuda' else torch.float32\n",
    "\n",
    "print(f'ğŸ–¥ï¸  Device : {DEVICE}')\n",
    "if DEVICE == 'cuda':\n",
    "    print(f'ğŸ®  GPU    : {torch.cuda.get_device_name(0)}')\n",
    "    print(f'ğŸ’¾  VRAM   : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('âš ï¸  No GPU detected â€” image generation will be very slow on CPU.')"
   ],
   "execution_count": null,
   "outputs": []
  },

  {
   "cell_type": "code",
   "metadata": {"id": "cell_pipelines"},
   "source": [
    "# â”€â”€ Cell 3: Load pipelines (lazy â€” loaded on first use) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_txt2img_pipe  = None\n",
    "_img2img_pipe  = None\n",
    "_video_pipe    = None\n",
    "\n",
    "SDXL_MODEL     = 'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "ANIMDIFF_MODEL = 'guoyww/animatediff-motion-adapter-v1-5-2'\n",
    "ANIMDIFF_BASE  = 'SG161222/Realistic_Vision_V5.1_noVAE'\n",
    "\n",
    "\n",
    "def get_txt2img():\n",
    "    global _txt2img_pipe\n",
    "    if _txt2img_pipe is None:\n",
    "        print('â³ Loading SDXL text-to-image pipeline...')\n",
    "        _txt2img_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            SDXL_MODEL,\n",
    "            torch_dtype=DTYPE,\n",
    "            use_safetensors=True,\n",
    "            variant='fp16' if DEVICE == 'cuda' else None,\n",
    "        ).to(DEVICE)\n",
    "        if DEVICE == 'cuda':\n",
    "            _txt2img_pipe.enable_xformers_memory_efficient_attention()\n",
    "        print('âœ… SDXL text-to-image ready')\n",
    "    return _txt2img_pipe\n",
    "\n",
    "\n",
    "def get_img2img():\n",
    "    global _img2img_pipe\n",
    "    if _img2img_pipe is None:\n",
    "        print('â³ Loading SDXL image-to-image pipeline...')\n",
    "        _img2img_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            SDXL_MODEL,\n",
    "            torch_dtype=DTYPE,\n",
    "            use_safetensors=True,\n",
    "            variant='fp16' if DEVICE == 'cuda' else None,\n",
    "        ).to(DEVICE)\n",
    "        if DEVICE == 'cuda':\n",
    "            _img2img_pipe.enable_xformers_memory_efficient_attention()\n",
    "        print('âœ… SDXL image-to-image ready')\n",
    "    return _img2img_pipe\n",
    "\n",
    "\n",
    "def get_video():\n",
    "    global _video_pipe\n",
    "    if _video_pipe is None:\n",
    "        print('â³ Loading AnimateDiff pipeline...')\n",
    "        adapter = MotionAdapter.from_pretrained(\n",
    "            ANIMDIFF_MODEL, torch_dtype=DTYPE\n",
    "        )\n",
    "        _video_pipe = AnimateDiffPipeline.from_pretrained(\n",
    "            ANIMDIFF_BASE,\n",
    "            motion_adapter=adapter,\n",
    "            torch_dtype=DTYPE,\n",
    "        ).to(DEVICE)\n",
    "        _video_pipe.scheduler = DDIMScheduler.from_config(\n",
    "            _video_pipe.scheduler.config,\n",
    "            beta_schedule='linear',\n",
    "            clip_sample=False,\n",
    "            timestep_spacing='linspace',\n",
    "            steps_offset=1,\n",
    "        )\n",
    "        if DEVICE == 'cuda':\n",
    "            _video_pipe.enable_vae_slicing()\n",
    "        print('âœ… AnimateDiff ready')\n",
    "    return _video_pipe\n",
    "\n",
    "\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print('âœ… Pipeline loaders defined')"
   ],
   "execution_count": null,
   "outputs": []
  },

  {
   "cell_type": "code",
   "metadata": {"id": "cell_generators"},
   "source": [
    "# â”€â”€ Cell 4: Generation functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    width,\n",
    "    height,\n",
    "    steps,\n",
    "    guidance_scale,\n",
    "    seed,\n",
    "):\n",
    "    \"\"\"Text â†’ Image using SDXL.\"\"\"\n",
    "    if not prompt.strip():\n",
    "        return None, 'âš ï¸ Please enter a prompt.'\n",
    "    try:\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(int(seed)) if seed >= 0 else None\n",
    "        pipe = get_txt2img()\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or None,\n",
    "            width=int(width),\n",
    "            height=int(height),\n",
    "            num_inference_steps=int(steps),\n",
    "            guidance_scale=float(guidance_scale),\n",
    "            generator=generator,\n",
    "        )\n",
    "        free_memory()\n",
    "        img = result.images[0]\n",
    "        info = (f'âœ… Generated {img.width}Ã—{img.height}  |  '\n",
    "                f'Steps: {steps}  |  CFG: {guidance_scale}  |  Seed: {seed}')\n",
    "        return img, info\n",
    "    except Exception as e:\n",
    "        return None, f'âŒ Error: {e}'\n",
    "\n",
    "\n",
    "def generate_img2img(\n",
    "    init_image,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    strength,\n",
    "    steps,\n",
    "    guidance_scale,\n",
    "    seed,\n",
    "):\n",
    "    \"\"\"Image + Prompt â†’ new Image using SDXL img2img.\"\"\"\n",
    "    if init_image is None:\n",
    "        return None, 'âš ï¸ Please upload an input image.'\n",
    "    if not prompt.strip():\n",
    "        return None, 'âš ï¸ Please enter a prompt.'\n",
    "    try:\n",
    "        # Resize to nearest 64-multiple\n",
    "        img = Image.fromarray(init_image).convert('RGB')\n",
    "        w = (img.width  // 64) * 64\n",
    "        h = (img.height // 64) * 64\n",
    "        img = img.resize((max(w, 512), max(h, 512)))\n",
    "\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(int(seed)) if seed >= 0 else None\n",
    "        pipe = get_img2img()\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or None,\n",
    "            image=img,\n",
    "            strength=float(strength),\n",
    "            num_inference_steps=int(steps),\n",
    "            guidance_scale=float(guidance_scale),\n",
    "            generator=generator,\n",
    "        )\n",
    "        free_memory()\n",
    "        out = result.images[0]\n",
    "        info = (f'âœ… Img2Img done  |  Strength: {strength}  |  '\n",
    "                f'Steps: {steps}  |  CFG: {guidance_scale}')\n",
    "        return out, info\n",
    "    except Exception as e:\n",
    "        return None, f'âŒ Error: {e}'\n",
    "\n",
    "\n",
    "def generate_video(\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    num_frames,\n",
    "    steps,\n",
    "    guidance_scale,\n",
    "    fps,\n",
    "    seed,\n",
    "):\n",
    "    \"\"\"Text â†’ Video (MP4) using AnimateDiff.\"\"\"\n",
    "    if not prompt.strip():\n",
    "        return None, 'âš ï¸ Please enter a prompt.'\n",
    "    try:\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(int(seed)) if seed >= 0 else None\n",
    "        pipe = get_video()\n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or None,\n",
    "            num_frames=int(num_frames),\n",
    "            num_inference_steps=int(steps),\n",
    "            guidance_scale=float(guidance_scale),\n",
    "            generator=generator,\n",
    "        )\n",
    "        free_memory()\n",
    "        frames = output.frames[0]  # list of PIL images\n",
    "\n",
    "        # Save to temp MP4\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False)\n",
    "        export_to_video(frames, tmp.name, fps=int(fps))\n",
    "        info = (f'âœ… Video: {len(frames)} frames @ {fps}fps  |  '\n",
    "                f'Steps: {steps}  |  CFG: {guidance_scale}')\n",
    "        return tmp.name, info\n",
    "    except Exception as e:\n",
    "        return None, f'âŒ Error: {e}'\n",
    "\n",
    "\n",
    "print('âœ… Generation functions defined')"
   ],
   "execution_count": null,
   "outputs": []
  },

  {
   "cell_type": "code",
   "metadata": {"id": "cell_ui"},
   "source": [
    "# â”€â”€ Cell 5: Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CSS = \"\"\"\n",
    ".gradio-container { max-width: 900px !important; margin: auto !important; }\n",
    "#title { text-align: center; margin-bottom: 8px; }\n",
    "#title h1 { font-size: 2em; font-weight: 800; }\n",
    "#title p  { color: #888; margin-top: 4px; }\n",
    ".status-box textarea { font-size: 13px !important; }\n",
    "\"\"\"\n",
    "\n",
    "NEG_DEFAULT = (\n",
    "    'blurry, low quality, ugly, deformed, watermark, text, '\n",
    "    'bad anatomy, extra limbs, duplicate'\n",
    ")\n",
    "\n",
    "with gr.Blocks(css=CSS, theme=gr.themes.Soft(), title='ğŸ¨ AI Generator') as demo:\n",
    "\n",
    "    gr.HTML('<div id=\"title\"><h1>ğŸ¨ AI Image &amp; Video Generator</h1>'\n",
    "            '<p>SDXL Â· AnimateDiff Â· Running on ' + DEVICE.upper() + '</p></div>')\n",
    "\n",
    "    # â”€â”€ Tab 1: Text â†’ Image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Tab('ğŸ–¼ï¸ Text â†’ Image'):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                t2i_prompt = gr.Textbox(\n",
    "                    label='Prompt',\n",
    "                    placeholder='A majestic wolf howling at the moon, digital art, 4K, dramatic lighting...',\n",
    "                    lines=3,\n",
    "                )\n",
    "                t2i_neg = gr.Textbox(\n",
    "                    label='Negative Prompt',\n",
    "                    value=NEG_DEFAULT,\n",
    "                    lines=2,\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    t2i_w      = gr.Slider(512, 1024, value=1024, step=64,  label='Width')\n",
    "                    t2i_h      = gr.Slider(512, 1024, value=1024, step=64,  label='Height')\n",
    "                with gr.Row():\n",
    "                    t2i_steps  = gr.Slider(10,  50,   value=30,   step=1,   label='Steps')\n",
    "                    t2i_cfg    = gr.Slider(1,   15,   value=7.5,  step=0.5, label='Guidance Scale')\n",
    "                with gr.Row():\n",
    "                    t2i_seed   = gr.Number(value=42, label='Seed (-1 = random)')\n",
    "                t2i_btn = gr.Button('âœ¨ Generate Image', variant='primary')\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                t2i_out    = gr.Image(label='Output', type='pil', height=420)\n",
    "                t2i_status = gr.Textbox(label='Status', interactive=False, elem_classes='status-box')\n",
    "\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                ['A futuristic cyberpunk city at night, neon lights, rain reflections, cinematic', NEG_DEFAULT, 1024, 1024, 30, 7.5, 42],\n",
    "                ['Portrait of an astronaut in a flower field, golden hour, photorealistic, 8K', NEG_DEFAULT, 1024, 1024, 30, 7.5, 7],\n",
    "                ['Underwater coral reef with tropical fish, vivid colors, National Geographic', NEG_DEFAULT, 1024, 768, 25, 7.0, 99],\n",
    "            ],\n",
    "            inputs=[t2i_prompt, t2i_neg, t2i_w, t2i_h, t2i_steps, t2i_cfg, t2i_seed],\n",
    "            label='Quick Examples',\n",
    "        )\n",
    "\n",
    "        t2i_btn.click(\n",
    "            fn=generate_image,\n",
    "            inputs=[t2i_prompt, t2i_neg, t2i_w, t2i_h, t2i_steps, t2i_cfg, t2i_seed],\n",
    "            outputs=[t2i_out, t2i_status],\n",
    "        )\n",
    "\n",
    "    # â”€â”€ Tab 2: Image â†’ Image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Tab('ğŸ”„ Image â†’ Image'):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                i2i_input  = gr.Image(label='Input Image', type='numpy', height=240)\n",
    "                i2i_prompt = gr.Textbox(\n",
    "                    label='Prompt',\n",
    "                    placeholder='Transform this into a watercolor painting...',\n",
    "                    lines=2,\n",
    "                )\n",
    "                i2i_neg    = gr.Textbox(label='Negative Prompt', value=NEG_DEFAULT, lines=2)\n",
    "                with gr.Row():\n",
    "                    i2i_strength = gr.Slider(0.1, 1.0, value=0.75, step=0.05, label='Strength (how much to change)')\n",
    "                    i2i_steps    = gr.Slider(10,  50,  value=30,   step=1,    label='Steps')\n",
    "                with gr.Row():\n",
    "                    i2i_cfg  = gr.Slider(1, 15, value=7.5, step=0.5, label='Guidance Scale')\n",
    "                    i2i_seed = gr.Number(value=42, label='Seed (-1 = random)')\n",
    "                i2i_btn = gr.Button('ğŸ”„ Transform Image', variant='primary')\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                i2i_out    = gr.Image(label='Output', type='pil', height=420)\n",
    "                i2i_status = gr.Textbox(label='Status', interactive=False, elem_classes='status-box')\n",
    "\n",
    "        i2i_btn.click(\n",
    "            fn=generate_img2img,\n",
    "            inputs=[i2i_input, i2i_prompt, i2i_neg, i2i_strength, i2i_steps, i2i_cfg, i2i_seed],\n",
    "            outputs=[i2i_out, i2i_status],\n",
    "        )\n",
    "\n",
    "    # â”€â”€ Tab 3: Text â†’ Video â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Tab('ğŸ¬ Text â†’ Video'):\n",
    "        gr.Markdown(\n",
    "            '> **AnimateDiff** generates short animated clips (~2â€“4 seconds). '\n",
    "            'Uses a separate SD 1.5-based model for best motion quality. '\n",
    "            'Requires ~6 GB VRAM â€” reduce frames if you hit OOM.'\n",
    "        )\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                vid_prompt = gr.Textbox(\n",
    "                    label='Prompt',\n",
    "                    placeholder='A golden retriever running through a meadow, slow motion, cinematic...',\n",
    "                    lines=3,\n",
    "                )\n",
    "                vid_neg   = gr.Textbox(label='Negative Prompt', value=NEG_DEFAULT, lines=2)\n",
    "                with gr.Row():\n",
    "                    vid_frames = gr.Slider(8,  32,  value=16,  step=4,   label='Frames')\n",
    "                    vid_fps    = gr.Slider(4,  16,  value=8,   step=1,   label='FPS')\n",
    "                with gr.Row():\n",
    "                    vid_steps  = gr.Slider(10, 50,  value=25,  step=1,   label='Steps')\n",
    "                    vid_cfg    = gr.Slider(1,  15,  value=7.5, step=0.5, label='Guidance Scale')\n",
    "                vid_seed  = gr.Number(value=42, label='Seed (-1 = random)')\n",
    "                vid_btn   = gr.Button('ğŸ¬ Generate Video', variant='primary')\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                vid_out    = gr.Video(label='Output Video', height=380)\n",
    "                vid_status = gr.Textbox(label='Status', interactive=False, elem_classes='status-box')\n",
    "\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                ['A campfire burning in a forest at night, sparks flying, cozy atmosphere', NEG_DEFAULT, 16, 8, 25, 7.5, 42],\n",
    "                ['Ocean waves crashing on a rocky shore, dramatic, cinematic', NEG_DEFAULT, 16, 8, 25, 7.5, 0],\n",
    "                ['Time-lapse of clouds moving over mountains, peaceful, 4K', NEG_DEFAULT, 24, 8, 30, 7.0, 5],\n",
    "            ],\n",
    "            inputs=[vid_prompt, vid_neg, vid_frames, vid_fps, vid_steps, vid_cfg, vid_seed],\n",
    "            label='Quick Examples',\n",
    "        )\n",
    "\n",
    "        vid_btn.click(\n",
    "            fn=generate_video,\n",
    "            inputs=[vid_prompt, vid_neg, vid_frames, vid_steps, vid_cfg, vid_fps, vid_seed],\n",
    "            outputs=[vid_out, vid_status],\n",
    "        )\n",
    "\n",
    "    # â”€â”€ Tab 4: Model Info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Tab('â„¹ï¸ Info'):\n",
    "        gr.Markdown(f\"\"\"\n",
    "## Models\n",
    "| Task | Model | Notes |\n",
    "|------|-------|-------|\n",
    "| Textâ†’Image | stabilityai/stable-diffusion-xl-base-1.0 | SDXL, 6.9B params |\n",
    "| Imageâ†’Image | stabilityai/stable-diffusion-xl-base-1.0 | Same model, img2img pipeline |\n",
    "| Textâ†’Video | SG161222/Realistic_Vision_V5.1_noVAE + AnimateDiff v1.5.2 | SD1.5-based |\n",
    "\n",
    "## Tips\n",
    "- **Quality prompts:** add `4K, highly detailed, professional photography, cinematic`\n",
    "- **Negative prompts:** always include deformity/quality negatives\n",
    "- **Guidance Scale:** 7â€“9 for balanced results; higher = more prompt-adherent\n",
    "- **Strength (img2img):** 0.3â€“0.5 for subtle changes, 0.7â€“0.9 for big transformations\n",
    "- **Video OOM?** Reduce frames from 16 to 8, or restart runtime\n",
    "\n",
    "## Hardware\n",
    "- Current device: **{DEVICE.upper()}**\n",
    "- Recommended: T4 GPU (free Colab tier) or A100 for video\n",
    "        \"\"\")\n",
    "\n",
    "print('âœ… UI built â€” launching...')\n",
    "demo.launch(share=True, debug=False)"
   ],
   "execution_count": null,
   "outputs": []
  }

 ]
}
